{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "667a1efa-b82c-4be3-b969-f4fd73f797ae",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <font color=\"darkblue\">\n",
    "    <span style=\"font-family: 'Times New Roman'; font-size: 22pt; line-height: 1.5;\">\n",
    "      <h1><b>Data Challenge</b></h1>\n",
    "    </span>\n",
    "  </font>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "  <font color=\"darkblue\">\n",
    "    <span style=\"font-family: 'Times New Roman'; font-size: 16pt; line-height: 1.5;\">\n",
    "      <h1><b>Web Scraping</b></h1>\n",
    "    </span>\n",
    "  </font>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "  <font color=\"darkblue\">\n",
    "    <span style=\"font-family: 'Times New Roman'; font-size: 12pt; line-height: 1.5;\">\n",
    "      <h1><i>Maaly Moulay El Hassen , 22240</i></h1>\n",
    "    </span>\n",
    "  </font>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b9cc4-a1dc-4e3c-8cc8-c536b427b69a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d1e7f2; padding: 20px; border-radius: 10px;\">\n",
    "    <font color=\"navy\">\n",
    "        <span style=\"font-family: 'Times New Roman'; font-size: 16pt;\">\n",
    "            <b>Voursa / Auto mauritanie / Occasion <b> </b>\n",
    "        </span>\n",
    "    </font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190476a-7905-4ca2-be52-eb41d019b5aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement à partir de https://www.voursa.com/index.cfm?PN=1&gct=1&sct=11&gv=13...\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/voursa\\accueil.gif\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/voursa\\voursa_boutiques_fr.gif\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/voursa\\LOGO.gif\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/voursa\\voursa_boutiques_ar2.gif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re\n",
    "\n",
    "def download_images(url, folder_path, user_agent=None):\n",
    "    headers = {'user-agent': user_agent} if user_agent else {}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Vérifie si la requête a réussi\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors de la récupération de la page: {e}\")\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    img_tags = soup.find_all('img')\n",
    "    \n",
    "    for img_tag in img_tags:\n",
    "        img_url = img_tag.get('src')\n",
    "        if img_url:\n",
    "            img_url = urljoin(url, img_url)\n",
    "            try:\n",
    "                img_response = requests.get(img_url, headers=headers)\n",
    "                img_response.raise_for_status()  # Vérifie si la requête a réussi\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Impossible de télécharger l'image à partir de {img_url}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            img_name = os.path.basename(urlparse(img_url).path)\n",
    "            img_name = re.sub(r'[^\\w\\s\\-_.]', '', img_name)\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            \n",
    "            try:\n",
    "                with open(img_path, 'wb') as img_file:\n",
    "                    img_file.write(img_response.content)\n",
    "                    print(f\"Image téléchargée: {img_path}\")\n",
    "            except IOError as e:\n",
    "                print(f\"Erreur lors de l'écriture de l'image {img_path}: {e}\")\n",
    "\n",
    "base_url = \"https://www.voursa.com/index.cfm?PN={}&gct=1&sct=11&gv=13\"\n",
    "folder_to_save_images = \"C:/Users/Dell/Desktop/Data Challenge/voursa\"\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n",
    "\n",
    "os.makedirs(folder_to_save_images, exist_ok=True)\n",
    "\n",
    "# Boucle de 1 à 26\n",
    "for i in range(1, 27):\n",
    "    url_to_scrape = base_url.format(i)\n",
    "    print(f\"Téléchargement à partir de {url_to_scrape}...\")\n",
    "    download_images(url_to_scrape, folder_to_save_images, user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a5d7f-46e2-499b-a378-68fc58db34e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a9cdb56-c476-486e-84b8-8af4d766a1a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\logo.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\20240312_150519-350x220.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\WhatsApp-Image-2024-03-06-at-10.34.33-350x220.jpeg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\5008-350x220.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\wrangler-3-350x220.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\8-350x220.jpeg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\pajero-DID-32-L-350x220.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\WhatsApp-Image-2023-09-16-at-11.08.32-1-350x220.jpeg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\IMG_4354-350x220.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\1-350x220.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\408D100E-FF36-49B9-BCE5-7C515598D413-350x220.jpeg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\571351DD-45E9-44BA-9629-9AA988C7AD50-350x220.jpeg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\IMG-20221001-WA0015-1-350x220.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\IMG_5180-350x220.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\Prado-350x220.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\20220924_134955-350x220.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\WhatsApp-Image-2022-08-29-at-17.55.18-350x220.jpeg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\PHOTO-2022-03-20-13-11-25-350x220.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-80x80.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\no-image-1-350x220.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\no-image-1-350x220.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\no-image-1-350x220.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\avatar-icon-images-4-1.png\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/expat-rim\\LOGO-V2-1Asset-1.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re\n",
    "# Fonction pour télécharger les images\n",
    "def download_images(url, folder_path, user_agent=None):\n",
    "    # Définir l'en-tête de l'agent utilisateur si spécifié\n",
    "    headers = {'user-agent': user_agent} if user_agent else {}\n",
    "    \n",
    "    # Récupérer le contenu HTML de la page\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Trouver toutes les balises 'img'\n",
    "    img_tags = soup.find_all('img')\n",
    "    \n",
    "    # Télécharger chaque image trouvée\n",
    "    for img_tag in img_tags:\n",
    "        img_url = img_tag.get('src')\n",
    "        if img_url:\n",
    "            # Combiner l'URL de l'image avec l'URL de la page pour obtenir l'URL absolue\n",
    "            img_url = urljoin(url, img_url)\n",
    "            \n",
    "            # Obtenir le nom de fichier à partir de l'URL et nettoyer les caractères invalides\n",
    "            img_name = os.path.basename(urlparse(img_url).path)\n",
    "            img_name = re.sub(r'[^\\w\\s\\-_.]', '', img_name)\n",
    "            \n",
    "            # Télécharger l'image\n",
    "            img_response = requests.get(img_url, headers=headers)\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            with open(img_path, 'wb') as img_file:\n",
    "                img_file.write(img_response.content)\n",
    "                print(f\"Image téléchargée: {img_path}\")\n",
    "\n",
    "# URL de la page à scraper\n",
    "url_to_scrape = \"https://www.expat-rim.com/ad_category/voitures-occasion-ou-neuves/\"\n",
    "folder_to_save_images = \"C:/Users/Dell/Desktop/Data Challenge/expat-rim\"\n",
    "\n",
    "# Agent utilisateur à utiliser\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n",
    "\n",
    "# Créer le dossier s'il n'existe pas déjà\n",
    "os.makedirs(folder_to_save_images, exist_ok=True)\n",
    "\n",
    "# Appeler la fonction pour télécharger les images en spécifiant l'agent utilisateur\n",
    "download_images(url_to_scrape, folder_to_save_images, user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9a0c03-11a1-4a65-8815-83369f3bd872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd3e7d-63a7-437d-bc76-f4b46a8833da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc458a-1fb8-40f9-a180-0a408079e127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8db4db51-0044-4048-9592-f167fb783405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/auto mauritanie/1\\image_1.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/auto mauritanie/1\\image_2.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/auto mauritanie/1\\image_3.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/auto mauritanie/1\\image_4.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/auto mauritanie/1\\image_5.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/auto mauritanie/1\\image_6.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/auto mauritanie/1\\image_7.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/auto mauritanie/1\\image_8.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/auto mauritanie/1\\image_9.jpg\n",
      "Image téléchargée: C:/Users/Dell/Desktop/Data Challenge/auto mauritanie/1\\image_10.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Fonction pour télécharger les images\n",
    "def download_images(url, folder_path, user_agent=None):\n",
    "    # Instancier le driver Selenium\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Exécuter Chrome en mode headless (sans interface utilisateur)\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    # Charger la page avec Selenium\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Attendre que la page soit entièrement chargée (peut être ajusté selon les besoins)\n",
    "    driver.implicitly_wait(10)\n",
    "    \n",
    "    # Extraire le contenu HTML après le chargement complet de la page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Trouver toutes les balises 'img'\n",
    "    img_tags = soup.find_all('img')\n",
    "    \n",
    "    # Compteur pour suivre le nombre d'images téléchargées\n",
    "    count = 0\n",
    "    \n",
    "    # Télécharger chaque image trouvée (jusqu'à 5 premières)\n",
    "    for img_tag in img_tags:\n",
    "        if count >= 10:\n",
    "            break  # Sortir de la boucle après avoir téléchargé 5 images\n",
    "        \n",
    "        img_url = img_tag.get('src')\n",
    "        if img_url:\n",
    "            # Obtenir le nom de fichier à partir de l'URL et nettoyer les caractères invalides\n",
    "            img_name = f\"image_{count + 1}.jpg\"  # Nommer chaque image de manière unique\n",
    "            img_name = \"\".join(x for x in img_name if x.isalnum() or x in [\" \", \"-\", \"_\", \".\"])\n",
    "            \n",
    "            # Télécharger l'image\n",
    "            img_response = requests.get(img_url)\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            with open(img_path, 'wb') as img_file:\n",
    "                img_file.write(img_response.content)\n",
    "                print(f\"Image téléchargée: {img_path}\")\n",
    "            \n",
    "            count += 1  # Incrémenter le compteur après chaque image téléchargée\n",
    "    \n",
    "    # Fermer le navigateur après le téléchargement des images\n",
    "    driver.quit()\n",
    "\n",
    "# URL de la page à scraper\n",
    "url_to_scrape = \"https://www.automauritanie.com/fr/vehicle_listings/annonce-nissan-murano-hodh-el-gharbi-aioun-3492\"\n",
    "folder_to_save_images = \"C:/Users/Dell/Desktop/Data Challenge/auto mauritanie/1\"\n",
    "\n",
    "# Créer le dossier s'il n'existe pas déjà\n",
    "os.makedirs(folder_to_save_images, exist_ok=True)\n",
    "\n",
    "# Appeler la fonction pour télécharger les 5 premières images\n",
    "download_images(url_to_scrape, folder_to_save_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b330dab-ecc8-4b72-b693-170d132d4823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c98d2772-521a-447f-bce4-569cb991875c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re\n",
    "\n",
    "def download_images(url, folder_path, user_agent=None):\n",
    "    headers = {'user-agent': user_agent} if user_agent else {}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors de la récupération de la page: {e}\")\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    img_tags = soup.find_all('img')\n",
    "    \n",
    "    for img_tag in img_tags:\n",
    "        img_url = img_tag.get('src')\n",
    "        if img_url:\n",
    "            img_url = urljoin(url, img_url)\n",
    "            try:\n",
    "                img_response = requests.get(img_url, headers=headers)\n",
    "                img_response.raise_for_status()\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Impossible de télécharger l'image à partir de {img_url}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            img_name = get_image_name(img_response, img_url)\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            \n",
    "            try:\n",
    "                with open(img_path, 'wb') as img_file:\n",
    "                    img_file.write(img_response.content)\n",
    "                    print(f\"Image téléchargée: {img_path}\")\n",
    "            except IOError as e:\n",
    "                print(f\"Erreur lors de l'écriture de l'image {img_path}: {e}\")\n",
    "\n",
    "def get_image_name(response, url):\n",
    "    content_disposition = response.headers.get('Content-Disposition')\n",
    "    if content_disposition and 'filename' in content_disposition:\n",
    "        filename = re.findall(\"filename=(.+)\", content_disposition)[0]\n",
    "    else:\n",
    "        filename = os.path.basename(urlparse(url).path)\n",
    "    return re.sub(r'[^\\w\\s\\-_.]', '', filename)\n",
    "\n",
    "base_url = \"https://www.automauritanie.com/petites-annonces-voitures-en-Adrar-Import%20-%20Dubai?_=1715637331563&page=4\"\n",
    "folder_to_save_images = \"C:/Users/Dell/Desktop/Data Challenge/voursa1\"\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n",
    "\n",
    "os.makedirs(folder_to_save_images, exist_ok=True)\n",
    "\n",
    "download_images(base_url, folder_to_save_images, user_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2051ba53-6d9e-4bfd-8842-48819d0231c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c1d3921-4b91-45e4-b4b0-3e71b98ae5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "# URL de la page avec les annonces de voitures\n",
    "url = 'https://www.automauritanie.com/fr/vehicle_listings/annonce-ford-mustang-hodh-ech-chargui-adel-bagrou-4611'\n",
    "\n",
    "# Téléchargez la page\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Trouvez tous les éléments d'image\n",
    "img_tags = soup.find_all('img')\n",
    "\n",
    "# Récupérez le domaine de l'URL de base pour construire les URL des images\n",
    "base_url = urlparse(url).scheme + '://' + urlparse(url).netloc\n",
    "\n",
    "# Répertoire de destination pour enregistrer les images\n",
    "destination_dir = destination_dir = \"C:/Users/Dell/Desktop/Data Challenge/voursa2\"\n",
    "\n",
    "# Créez le répertoire s'il n'existe pas\n",
    "if not os.path.exists(destination_dir):\n",
    "    os.makedirs(destination_dir)\n",
    "\n",
    "# Téléchargez chaque image\n",
    "for img in img_tags:\n",
    "    img_url = img['src']\n",
    "    if not img_url.startswith(('http://', 'https://')):\n",
    "        img_url = urljoin(base_url, img_url)\n",
    "    img_name = os.path.basename(img_url)\n",
    "    if img_url.lower().endswith(('.png', '.jpg')):\n",
    "        img_path = os.path.join(destination_dir, img_name)\n",
    "        with open(img_path, 'wb') as f:\n",
    "            img_data = requests.get(img_url).content\n",
    "            f.write(img_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eaa458-73f5-4dbf-976a-c83e29e70fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d65b1e-b30b-48fc-899a-6d12d636cb99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to download images from all pages starting from a specific URL\n",
    "def download_images_from_specific_url(start_url, base_url, total_pages, save_dir):\n",
    "    # Initialize counter for image filenames\n",
    "    image_counter = 1\n",
    "    \n",
    "    # List of alt attributes to exclude\n",
    "    excluded_alt = [\"Whatsapp\",\"v_logo\", \"TOYOTA\", \"VS\", \"toyota\", \"Share on Facebook\", \"rss-icon\", \"voursa boutiques\", \"Menu\", \"voursa\", \"\"]\n",
    "    \n",
    "    # Flag to indicate when to start downloading\n",
    "    start_downloading = False\n",
    "    \n",
    "    # Iterate through each page\n",
    "    for page_num in range(1, total_pages + 1):\n",
    "        # Generate the URL for the current page\n",
    "        page_url = base_url + f\"&PN={page_num}\" if page_num > 1 else base_url\n",
    "        \n",
    "        # Send HTTP GET request to fetch the page content\n",
    "        response = requests.get(page_url)\n",
    "        \n",
    "        # Check if request was successful\n",
    "        if response.status_code == 200:\n",
    "            page_content = response.content\n",
    "            soup = BeautifulSoup(page_content, 'html.parser')\n",
    "            \n",
    "            # Find all <a> tags\n",
    "            links = soup.find_all('a')\n",
    "            \n",
    "            # Filter links to include only those with href starting with \"annonces.cfm?\"\n",
    "            valid_links = [link['href'] for link in links if link.has_attr('href') and link['href'].startswith(\"/annonces.cfm?\")]\n",
    "            \n",
    "            # Iterate through each valid URL\n",
    "            for annonce_url in valid_links:\n",
    "                full_url = urljoin(\"https://www.voursa.com/\", annonce_url)\n",
    "                print(\"Processing URL:\", full_url)\n",
    "                \n",
    "                # Check if the current URL matches the start URL\n",
    "                if full_url == start_url:\n",
    "                    start_downloading = True\n",
    "                \n",
    "                # Start downloading images when the start URL is reached\n",
    "                if start_downloading:\n",
    "                    # Send HTTP GET request to fetch the announcement page content\n",
    "                    annonce_response = requests.get(full_url)\n",
    "                    \n",
    "                    if annonce_response.status_code == 200:\n",
    "                        annonce_content = annonce_response.content\n",
    "                        annonce_soup = BeautifulSoup(annonce_content, 'html.parser')\n",
    "                        \n",
    "                        # Find all image URLs from the announcement webpage\n",
    "                        image_tags = annonce_soup.find_all('img', src=True)\n",
    "                        \n",
    "                        # Filter image nodes based on alt attribute not containing excluded values\n",
    "                        relevant_images = [image['src'] for image in image_tags if image['alt'] not in excluded_alt]\n",
    "                        \n",
    "                        print(\"Number of relevant images:\", len(relevant_images))\n",
    "                        \n",
    "                        # Download and save all the relevant images\n",
    "                        for image_url in relevant_images:\n",
    "                            # Generate filename with counter\n",
    "                            filename = f\"web_{image_counter:03d}.png\"\n",
    "                            \n",
    "                            # Parse the image URL\n",
    "                            parsed_url = urlparse(image_url)\n",
    "                            \n",
    "                            # Construct a valid URL if it's malformed\n",
    "                            if not parsed_url.scheme or not parsed_url.netloc:\n",
    "                                image_url = \"https://www.voursa.com\" + image_url\n",
    "                            \n",
    "                            # Send HTTP GET request to download the image\n",
    "                            image_response = requests.get(image_url)\n",
    "                            \n",
    "                            if image_response.status_code == 200:\n",
    "                                # Save the image to the specified directory\n",
    "                                image_path = os.path.join(save_dir, filename)\n",
    "                                with open(image_path, 'wb') as f:\n",
    "                                    f.write(image_response.content)\n",
    "                                \n",
    "                                print(f\"Image saved: {image_path}\")\n",
    "                                # Increment image counter\n",
    "                                image_counter += 1\n",
    "                            else:\n",
    "                                print(f\"Error downloading image: {image_response.status_code}\")\n",
    "                    \n",
    "                    else:\n",
    "                        print(f\"Error fetching announcement page: {annonce_response.status_code}\")\n",
    "    \n",
    "    print(\"All images downloaded successfully\")\n",
    "\n",
    "# Specify the start URLProcessing URL: \n",
    "\n",
    "start_url = \"https://www.voursa.com/annonces.cfm?pdtid=128004&adtre=Wolswagen jetta gasoil \"\n",
    "# Example base URL and total number of pages\n",
    "base_url = \"https://www.voursa.com/index.cfm?gct=1&sct=11&gv=13\"\n",
    "total_pages = 26  # Assuming there are 26 pages\n",
    "save_directory = \"C:/Users/Dell/Desktop/Data Challenge/voursa1000\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Apply the function to scrape images from all pages starting from the specified URL and save them in the specified directory\n",
    "download_images_from_specific_url(start_url, base_url, total_pages, save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33ef8e-5607-4fa0-987b-e60505982b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76bad5de-041a-4f3e-97fb-0a7cf6e33061",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d1e7f2; padding: 20px; border-radius: 10px;\">\n",
    "    <font color=\"navy\">\n",
    "        <span style=\"font-family: 'Times New Roman'; font-size: 16pt;\">\n",
    "            <b> Facebook <b> </b>\n",
    "        </span>\n",
    "    </font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c414fe-79d0-4331-a29c-e8e08810cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Dossier où les photos seront enregistrées\n",
    "output_folder = r\"C:/Users/Dell/Desktop/Data Challenge/face\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Configuration de Selenium WebDriver\n",
    "driver = webdriver.Chrome()  # Assurez-vous que chromedriver est dans votre PATH ou spécifiez le chemin complet\n",
    "\n",
    "# Ouvrir la page Facebook\n",
    "driver.get(\"https://www.facebook.com/marketplace/108144375883042/vehicles?_rdc=1&_rdr\")\n",
    "\n",
    "# Attendre que la page charge (ajustez le délai si nécessaire)\n",
    "time.sleep(1000)  # Ajustez selon les besoins pour laisser le temps à la page de se charger\n",
    "\n",
    "# Récupérer le contenu de la page\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Extraire les URLs des images\n",
    "images = soup.find_all('img')\n",
    "image_urls = [img['src'] for img in images if 'src' in img.attrs and not img['src'].startswith('data:image')]\n",
    "\n",
    "# Télécharger les images\n",
    "for i, img_url in enumerate(image_urls):\n",
    "    try:\n",
    "        img_data = requests.get(img_url).content\n",
    "        img_filename = os.path.join(output_folder, f'image_{i+1}.jpg')\n",
    "        with open(img_filename, 'wb') as img_file:\n",
    "            img_file.write(img_data)\n",
    "        print(f\"Downloaded {img_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not download {img_url}: {e}\")\n",
    "\n",
    "# Fermer le WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e801ab6-5ada-4773-ac37-7b45df1b2ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd2528a3-260d-42d5-a8f9-70d0ca7a570a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d1e7f2; padding: 20px; border-radius: 10px;\">\n",
    "    <font color=\"navy\">\n",
    "        <span style=\"font-family: 'Times New Roman'; font-size: 16pt;\">\n",
    "            <b>Renommer les images <b> </b>\n",
    "        </span>\n",
    "    </font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b86346d-a236-4b7d-8c0a-01d1693c0a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Chemin du dossier contenant les photos\n",
    "dossier_photos = \"C:/Users/Dell/Desktop/Data Challenge/web scraping\"\n",
    "# Liste des fichiers dans le dossier\n",
    "fichiers = os.listdir(dossier_photos)\n",
    "\n",
    "# Tri des fichiers par date de modification\n",
    "fichiers_tries = sorted(fichiers, key=lambda x: os.path.getmtime(os.path.join(dossier_photos, x)))\n",
    "\n",
    "# Renommage des fichiers dans l'ordre\n",
    "for i, fichier in enumerate(fichiers_tries):\n",
    "    nom, extension = os.path.splitext(fichier)\n",
    "    nouveau_nom = f\"web1_maaly{i+1}{extension}\"  # Utilisation d'un compteur numérique avec préfixe \"web\"\n",
    "    os.rename(os.path.join(dossier_photos, fichier), os.path.join(dossier_photos, nouveau_nom))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4866df5-0e8c-44a2-aa47-332ff658db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Chemin du dossier contenant les photos\n",
    "dossier_photos = \"C:/Users/Dell/Desktop/Data Challenge/web scraping\"\n",
    "# Liste des fichiers dans le dossier\n",
    "fichiers = os.listdir(dossier_photos)\n",
    "\n",
    "# Tri des fichiers par date de modification\n",
    "fichiers_tries = sorted(fichiers, key=lambda x: os.path.getmtime(os.path.join(dossier_photos, x)))\n",
    "\n",
    "# Renommage des fichiers dans l'ordre\n",
    "for i, fichier in enumerate(fichiers_tries):\n",
    "    nom, extension = os.path.splitext(fichier)\n",
    "    nouveau_nom = f\"web1_maaly_{uuid.uuid4().hex}{extension}\"  # Utilisation d'un UUID unique comme suffixe\n",
    "    os.rename(os.path.join(dossier_photos, fichier), os.path.join(dossier_photos, nouveau_nom))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c888e1-f8d8-4473-8082-8e13109a6640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
